<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
<HEAD>
<META NAME="GENERATOR" CONTENT="Microsoft FrontPage 4.0">
<TITLE>Naive Bayes Learning</TITLE>
<STYLE>
<!--
BODY {font-family: MS Sans Serif; font-size: 10pt }
H1 {text-align: left; font-family: MS Sans Serif; font-size: 12pt; font-weight: medium; font-style: normal }
H2 {text-align: left; font-family: MS Sans Serif; font-size: 11pt; font-weight: medium; font-style: normal }
H3 {text-align: left; font-family: MS Sans Serif; font-size: 10pt; font-weight: medium; font-style: normal }
-->
</STYLE>
</HEAD>
<BODY BGCOLOR=#ffffff>

<h1><u><font size="6"><b>Naive Bayes Learning</b></font></u></h1>
<hr>
<p class="MsoBodyText" style="TEXT-INDENT: 35.4pt" align="left">A aprendizagem Bayesiana dá
uma visão probabilística à inferência. Ela é baseada na idéia de que instâncias
de interesse são governadas por distribuições de probabilidade e que decisões
extras podem ser feitas medindo estas probabilidades junto ao conjunto de
treinamento. Esta aprendizagem é interessante para machine learning dado que
ela provê uma base para algoritmos de aprendizagem que manipulam probabilidades
diretamente e cria um framework para a análise da operação de outros
algoritmos que não manipulam probabilidades diretamente.</p>
<p class="MsoBodyText" style="TEXT-INDENT: 35.4pt" align="left">Uma dificuldade prática em
aplicar métodos bayesianos é que eles tipicamente requerem conhecimento
inicial de muitas probabilidades. Quando estas probabilidades não são
conhecidas à priori elas devem ser estimadas baseadas em alguma experiência
anterior, dados disponíveis anteriormente, ou ainda, em alguma suposição dada
na forma de distribuições estatísticas. Uma segunda dificuldade prática é o
significante custo computacional para uma hipótese bayesiana ótima no caso
geral. Em algumas situações específicas este custo computacional pode ser
significantemente reduzido.</p>
<p class="MsoNormal" align="left">O classificador Naive Bayes</p>
<p class="MsoNormal" align="left">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
O algoritmo naive bayes é baseado no Teorema de Bayes e é ótimo para
probabilidades múltiplas quando os eventos são independentes. Assumir que os
atributos de uma classe são independentes no conjunto de treinamento é muito
simplista, pois o que os torna interessante é que os atributos apresentam
diferentes relacionamentos levando a um esquema simples que trabalhe bem na prática.</p>
<p class="MsoNormal" style="TEXT-INDENT: 35.4pt" align="left">Embora exista
esta desvantagem o classificador naive bayes funciona muito bem na prática,
principalmente quando combinado com outros algoritmos.</p>
<p class="MsoNormal" style="TEXT-INDENT: 35.4pt" align="left">Uma diferença
interessante entre o naive bayes e os outros métodos de aprendizagem
considerados é que não há uma procura explícita pelo espaço de hipóteses
possíveis. A hipótese é formada sem procura, simplesmente obtêm-se a hipótese
contando a freqüência de várias combinações de dados em um conjunto de
treinamento.</p>
</BODY>
</HTML>